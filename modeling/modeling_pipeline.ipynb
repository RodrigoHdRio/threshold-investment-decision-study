{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline\n",
    "Split, train models, and evaluate decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_time(modeling_panel, feature_columns, train_fraction=0.80):\n",
    "    available_months = np.array(sorted(modeling_panel[\"month_end\"].unique()))\n",
    "    if len(available_months) < 5:\n",
    "        raise ValueError(\"Not enough monthly observations for a time-based split.\")\n",
    "\n",
    "    split_index = int(len(available_months) * train_fraction) - 1\n",
    "    split_month = available_months[split_index]\n",
    "\n",
    "    train_mask = modeling_panel[\"month_end\"] <= split_month\n",
    "    test_mask = modeling_panel[\"month_end\"] > split_month\n",
    "\n",
    "    train_features = modeling_panel.loc[train_mask, feature_columns]\n",
    "    train_target = modeling_panel.loc[train_mask, \"target_return\"]\n",
    "    test_features = modeling_panel.loc[test_mask, feature_columns]\n",
    "    test_target = modeling_panel.loc[test_mask, \"target_return\"]\n",
    "\n",
    "    if train_features.empty or test_features.empty:\n",
    "        raise ValueError(\"Train/test split produced an empty set.\")\n",
    "\n",
    "    return split_month, train_features, train_target, test_features, test_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_features, train_target):\n",
    "    linear_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression()),\n",
    "    ])\n",
    "    random_forest_model = RandomForestRegressor(random_state=42, n_jobs=1)\n",
    "\n",
    "    linear_model.fit(train_features, train_target)\n",
    "    random_forest_model.fit(train_features, train_target)\n",
    "\n",
    "    return linear_model, random_forest_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_returns(linear_model, random_forest_model, test_features):\n",
    "    linear_predictions = pd.Series(\n",
    "        linear_model.predict(test_features),\n",
    "        index=test_features.index,\n",
    "        name=\"linear_prediction\",\n",
    "    )\n",
    "\n",
    "    random_forest_predictions = pd.Series(\n",
    "        random_forest_model.predict(test_features),\n",
    "        index=test_features.index,\n",
    "        name=\"random_forest_prediction\",\n",
    "    )\n",
    "\n",
    "    return linear_predictions, random_forest_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(actual_returns, approve_mask):\n",
    "    return {\n",
    "        \"approved_count\": int(approve_mask.sum()),\n",
    "        \"rejected_count\": int((~approve_mask).sum()),\n",
    "        \"type_I_errors\": int((approve_mask & (actual_returns < 0)).sum()),\n",
    "        \"type_II_errors\": int((~approve_mask & (actual_returns >= 0)).sum()),\n",
    "        \"approved_return_sum\": float(actual_returns[approve_mask].sum()),\n",
    "        \"rejected_return_sum\": float(actual_returns[~approve_mask].sum()),\n",
    "        \"approved_return_avg\": float(actual_returns[approve_mask].mean()) if approve_mask.any() else np.nan,\n",
    "        \"rejected_return_avg\": float(actual_returns[~approve_mask].mean()) if (~approve_mask).any() else np.nan,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_decisions(actual_returns, linear_predictions, random_forest_predictions, thresholds):\n",
    "    decision_records = []\n",
    "    flip_records = []\n",
    "\n",
    "    observation_count = len(actual_returns)\n",
    "    for threshold in thresholds:\n",
    "        linear_approve = linear_predictions >= threshold\n",
    "        random_forest_approve = random_forest_predictions >= threshold\n",
    "\n",
    "        disagreement_mask = linear_approve != random_forest_approve\n",
    "        flip_records.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"flip_rate\": disagreement_mask.mean() if observation_count else np.nan,\n",
    "            \"flips_total\": int(disagreement_mask.sum()),\n",
    "            \"linear_approve_rf_reject\": int((linear_approve & ~random_forest_approve).sum()),\n",
    "            \"rf_approve_linear_reject\": int((~linear_approve & random_forest_approve).sum()),\n",
    "        })\n",
    "\n",
    "        for model_name, approve_mask in [\n",
    "            (\"LinearRegression\", linear_approve),\n",
    "            (\"RandomForest\", random_forest_approve),\n",
    "        ]:\n",
    "            metrics = evaluate_threshold(actual_returns, approve_mask)\n",
    "            metrics.update({\"model\": model_name, \"threshold\": threshold})\n",
    "            decision_records.append(metrics)\n",
    "\n",
    "    decision_results = pd.DataFrame(decision_records)\n",
    "    flip_results = pd.DataFrame(flip_records)\n",
    "    return decision_results, flip_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_tables(decision_results, flip_results):\n",
    "    ordered_results = decision_results.sort_values([\"threshold\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "    detailed_column_order = [\n",
    "        \"model\",\n",
    "        \"threshold\",\n",
    "        \"approved_count\",\n",
    "        \"rejected_count\",\n",
    "        \"type_I_errors\",\n",
    "        \"type_II_errors\",\n",
    "        \"approved_return_sum\",\n",
    "        \"rejected_return_sum\",\n",
    "        \"approved_return_avg\",\n",
    "        \"rejected_return_avg\",\n",
    "    ]\n",
    "    ordered_results = ordered_results[detailed_column_order]\n",
    "\n",
    "    flip_summary_table = flip_results.copy()\n",
    "    flip_summary_table[\"flip_rate\"] = flip_summary_table[\"flip_rate\"].round(4)\n",
    "\n",
    "    decision_error_table = (\n",
    "        ordered_results\n",
    "        .pivot(\n",
    "            index=\"threshold\",\n",
    "            columns=\"model\",\n",
    "            values=[\"approved_count\", \"rejected_count\", \"type_I_errors\", \"type_II_errors\"],\n",
    "        )\n",
    "        .sort_index(axis=1, level=[0, 1])\n",
    "    )\n",
    "\n",
    "    impact_table = (\n",
    "        ordered_results\n",
    "        .pivot(\n",
    "            index=\"threshold\",\n",
    "            columns=\"model\",\n",
    "            values=[\"approved_return_sum\", \"rejected_return_sum\"],\n",
    "        )\n",
    "        .sort_index(axis=1, level=[0, 1])\n",
    "        .round(6)\n",
    "    )\n",
    "\n",
    "    average_impact_table = (\n",
    "        ordered_results\n",
    "        .pivot(\n",
    "            index=\"threshold\",\n",
    "            columns=\"model\",\n",
    "            values=[\"approved_return_avg\", \"rejected_return_avg\"],\n",
    "        )\n",
    "        .sort_index(axis=1, level=[0, 1])\n",
    "        .round(6)\n",
    "    )\n",
    "\n",
    "    return ordered_results, flip_summary_table, decision_error_table, impact_table, average_impact_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modeling_pipeline(modeling_panel):\n",
    "    split_month, train_features, train_target, test_features, test_target = split_train_test_by_time(\n",
    "        modeling_panel,\n",
    "        MODEL_FEATURE_COLUMNS,\n",
    "    )\n",
    "\n",
    "    linear_model, random_forest_model = train_models(train_features, train_target)\n",
    "    linear_predictions, random_forest_predictions = predict_returns(\n",
    "        linear_model,\n",
    "        random_forest_model,\n",
    "        test_features,\n",
    "    )\n",
    "\n",
    "    decision_results, flip_results = evaluate_model_decisions(\n",
    "        test_target,\n",
    "        linear_predictions,\n",
    "        random_forest_predictions,\n",
    "        DECISION_THRESHOLDS,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        detailed_results,\n",
    "        flip_summary_table,\n",
    "        decision_error_table,\n",
    "        impact_table,\n",
    "        average_impact_table,\n",
    "    ) = build_summary_tables(decision_results, flip_results)\n",
    "\n",
    "    print(f\"Train rows: {len(train_features)} | Test rows: {len(test_features)}\")\n",
    "    print(f\"Split month: {pd.to_datetime(split_month).date()}\")\n",
    "\n",
    "    print(\"Table 1. Flip Summary by Threshold\")\n",
    "    display(flip_summary_table)\n",
    "\n",
    "    print(\"Table 2. Decision Counts and Error Counts (Side-by-Side)\")\n",
    "    display(decision_error_table)\n",
    "\n",
    "    print(\"Table 3. Economic Impact (Realized Return Sums, Side-by-Side)\")\n",
    "    display(impact_table)\n",
    "\n",
    "    print(\"Table 4. Average Realized Returns (Side-by-Side)\")\n",
    "    display(average_impact_table)\n",
    "\n",
    "    print(\"Detailed Long-Format Results\")\n",
    "    display(detailed_results)\n",
    "\n",
    "    return {\n",
    "        \"split_month\": split_month,\n",
    "        \"train_features\": train_features,\n",
    "        \"train_target\": train_target,\n",
    "        \"test_features\": test_features,\n",
    "        \"test_target\": test_target,\n",
    "        \"linear_model\": linear_model,\n",
    "        \"random_forest_model\": random_forest_model,\n",
    "        \"linear_predictions\": linear_predictions,\n",
    "        \"random_forest_predictions\": random_forest_predictions,\n",
    "        \"detailed_results\": detailed_results,\n",
    "        \"flip_summary_table\": flip_summary_table,\n",
    "        \"decision_error_table\": decision_error_table,\n",
    "        \"impact_table\": impact_table,\n",
    "        \"average_impact_table\": average_impact_table,\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}